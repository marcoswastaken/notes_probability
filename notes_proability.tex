\documentclass[10pt]{article}

\usepackage{amsmath, amssymb}
\usepackage{multicol}

\usepackage[margin=.5in]{geometry}

\pagestyle{empty}
\begin{document}
		
		\section{Probability}
		
		\subsection{Combinatorial Analysis}
		
		The \emph{basic principle of counting}: If an experiment consists of two phases, with $n$ possible outcomes in the first phase, and $m$ possible outcomes in the second phase for each of those outcomes, then the total possible number of outcomes is $nm$.
		
		There are $n!$ possible linear orderings of $n$ items.
		
		The number of distinct ways to choose $k$ items from $n$ without regard to order is given by
		
		\begin{align*}
			{n \choose k} = \frac{n!}{(n-k)!k!}
		\end{align*}
		
		For non-negative integers $n_1,n_2,\ldots, n_r$ with $\sum n_i = n$, the number distinct of ways to divide $n$ items into non-overlapping groups of sizes $n_1, n_2,\ldots, n_r$ is given by
		
		\begin{align*}
			{n \choose {n_1, n_2,\ldots, n_r}} = \frac{n!}{n_1!n_2! \cdots n_r!}
		\end{align*}
		
		\subsection{Axioms of Probability}
		
		Some notation and terminology:
		
		\begin{itemize}
			\item $S$ is the set of all possible outcomes of an experiment, a.k.a. the \emph{sample space}
			\item An \emph{event} is a subset of $S$
			\item For any event $A$ we define $A^c$ to be the complent of $A$ in $S$
			\item $S^c=\emptyset$ is the null set
			\item $A\cap B$ or sometimes $AB$ is the intersection of the sets $A$ and $B$. If $A\cap B = \emptyset$ then we say that $A$ and $B$ are mutually exclusive
		\end{itemize}
		
		\subsubsection*{Axioms}
		
		If $E$ is an event in a sample space $S$ then the probability that event $E$ occurs as the outcome of a single experiment is denoted by $P(E)$ and satisfies the following axioms
		
		\begin{enumerate}
			\item $0 \leq P(E) \leq 1$
			\item $P(S)=1$
			\item If $\left\{ E_i \right\}_{i=1}^\infty$ is a set of pairwise mutually exclusive events then $P(\cup_{i=1}^\infty)=\sum_{i=1}^\infty P(E_i)$
		\end{enumerate}
		
		\subsubsection*{Useful Results}
		
		\begin{align*}
			P(A^c) = 1 - P(A)
		\end{align*}
		
		\begin{align*}
			P(A \cup B) = P(A) + P(B) - P(A \cap B)
		\end{align*}
		
		\noindent Informally, the generalization of this formula: add up all of the individual event probabilities, then subtract all of the double counted intersections, then add back all of the twice removed triple intersections, then remove all of the twice restored double counted quadruple intersections, etc.
		
		If $S$ is finite and every point in $S$ is assumed to have equal probability of occurring then
		
		\begin{align*}
			 P(A) = \frac{|A|}{|S|}
		\end{align*}
		
		\subsection{Conditional Probability and Independence}
		
		\subsubsection*{Conditional Probability}
		
		For events $E$ and $F$, the conditional probability of $E$ occurs, given that $F$ has occurred is denoted by  $P(E|F)$ and is defined by
		\begin{align*}
			P(E|F) 
				= \frac{P(E\cap F)}{P(F)}
				= \frac{P(F|E)P(E)}{P(F)}
				= \frac{P(F|E)P(E)}{P(F|E)P(E)+P(F|E^c)P(E^c)}
		\end{align*}
		where the last expression is \emph{Bayes's Formula}
		
		\subsubsection*{Independence}
		
		Two events are said to be \emph{independent} if $P(E\cap F) = P(E)P(F)$. Events that are not independent are said to be \emph{dependent}.  This generalizes to sets of events if every the probability of intersection of subsets is equal to the product of the probabilities of the individual events in the subset.
		
		\subsection{Random Variables}
		
		\subsubsection*{Random Variables}
		
		Given an experiment with sample space $S$, a \emph{random variable} is a real-valued function, $X$, defined on $S$. Because the value of a random variable is determined by the outcome of the experiment, we can assign probabilities to the possible values of the random variable, denoted by $P(X=x)$.
		
		\subsubsection*{Discrete Random Variables}
		If the range of $X$ is countable, $\left\{ x_1, x_2, \ldots \right\}$, then $X$ is said to be discrete. In this case we can define the \emph{probability mass function} $p:\mathbb{R}\to\mathbb{R}$ of $X$ to be $p(a) = P(X=a)$ which has the following properties:
		\begin{multicols}{3}
			\begin{enumerate}
				\item $p(x_i) \geq 0$ for $i=1,2,\ldots$
				\item $p(x) = 0$ for any other values of $x$
				\item $\sum_{i=1}^{\infty}p(x_i) = 1$
			\end{enumerate}
		\end{multicols}
		
		\subsubsection*{Expected Value}
		
		If $X$ is a discrete random variable with probability mass function $p(x)$ then the \emph{expected value} of $X$, denoted as $E[X]$ is defined by
		
		\begin{align*}
			E[X] = \sum_{xp(x)>0}xp(x)
		\end{align*}
		
		\noindent Some useful properties, for a random variable $X$ with non-zero outputs $\left\{ x_1, x_2, \ldots \right\}$: 
		
		\begin{itemize}
			\item If $g$ is any real value function then $E[g(X)] =  \sum_{i=1}^{\infty}g(x_i)p(x_i)$
			\item If $a$ and $b$ are constants, then $E[aX+b] = aE[X] + b $
		\end{itemize}
		
		\subsubsection*{Variance, Standard Deviation, Covariance, and Correlation}
		
		If $X$ is a random variable with \emph{mean} $\mu=E[X]$ then the variance of $X$, denoted by $\text{Var}(X)$, is defined to be
		
		\begin{align*}
			\text{Var}(X) = E[(X-\mu)^2]
		\end{align*}
		
		\noindent Some useful formulas that follow from this definition:
		\begin{multicols}{2}
			\begin{itemize}
				\item $\text{Var}(X) = E[X^2] - (E[X])^2$
				\item $\text{Var}(aX + b) = a^2 \text{Var}(x)$
			\end{itemize}
		\end{multicols}
		
		\noindent The \emph{standard deviation} of $X$ is the square root of the variance: $\sigma = \sqrt{\text{Var}(X)}$
		
		\noindent If $X$ and $Y$ are random variables, then the \emph{covariance} between them is $\text{Cov}(X,Y)=E[XY]-E[X]E[Y]$ and the \emph{correlation} between them is $\text{Cor}(X,Y) =\frac{ \text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}$. Units of covariance are the products of the units of $X$ and $Y$, while correlation is unit-less.
		
		\subsubsection*{Bernoulli  Random Variable}
		
		A Bernoulli trial is an experiment that has two outcomes: \emph{success} or \emph{failure}. The sample space is $S=\left\{ \text{success}, \text{failure}\right\}$ and a \emph{Bernoulli random variable}, $X$, is defined by $X(\text{success})=1$ and $X(\text{failure}) = 0$, along with the probability mass function is given by
		\begin{align*}
			p(0) = P(X=0) = 1 - p  && p(0) = P(X=1) = p
		\end{align*}
		
		\noindent where $p$ is the probability of success in a given trial.
		\subsubsection*{Binomial Random Variable}
		A \emph{binomial random variable} is a random variable $X$ that represents the number of successes that occur in $n$ Bernoulli trials, each with probability of success $p$.
		
		\noindent Properties of Binomial Random Variables:
		\begin{multicols}{3}
			\begin{itemize}
				\item $P(X=k) = {n \choose k} p^{k} (1-p)^{n-k}$
				\item $E[X] = np$
				\item $\text{Var}(X)= np(1-p) $
			\end{itemize}
		\end{multicols}
		
		
		\subsubsection*{Poisson Random Variable}
		
		A \emph{Poisson Random Variable} is a random variable $X$ that takes on one of the values $0, 1, 2, \ldots$, or the number of successes that occur in a given interval, together with a parameter $\gamma > 0$ that describes the expected number of success in that interval.
		
		\noindent Properties of Poisson Random Variables:
		
		\begin{multicols}{3}
			\begin{itemize}
				\item $P(X=k) = e^{-\gamma} \: \dfrac{\gamma^k}{k!}$
				\item $E[X] = \gamma$
				\item $\text{Var}(X) = \gamma$
			\end{itemize}
		\end{multicols}
	
		\subsubsection*{Geometric Random Variable}
		A \emph{Geometric Random Variable} is a random variable $X$ that takes on one of the values $0, 1, 2, \ldots$, or the number of trials required until a success occurs, when repeatedly performing a Bernoulli trial with probability of success $p$.
		
		\noindent Properties of Geometric Random Variables:
		\begin{multicols}{3}
			\begin{itemize}
				\item $P(X=k) = p(1-p)^{k-1}$
				\item $E[X] = \frac{1}{p}$
				\item $\text{Var}(X)= \frac{1-p}{p^2} $
			\end{itemize}
		\end{multicols}
		
		\subsubsection*{Negative Binomial Random Variable}
		
		Suppose that independent Bernoulli trials, with proability of success $p$, are perfomed until $r$ successes occur. A \emph{negative binomial random variable} is a random variable $X$ that takes on one of the values $0, 1, 2, \ldots$, or the number of trials required.
		\noindent Properties of Negative Binomial Random Variables:
		
		\begin{multicols}{3}
			\begin{itemize}
				\item $P(X=k) = {{k-1} \choose {r-1}}p^r(1-p)^{k-r}$
				\item $E[X] = \frac{r}{p}$
				\item $\text{Var}(X)= \frac{r(1-p)}{p^2} $
			\end{itemize}
		\end{multicols}
		
		\subsubsection*{Hypergeometric Random Variables}
		A \emph{Hypergeometric Random Variable} is a random variable $X$ that takes on one of the values $0, 1, 2, \ldots$, describing the number of successes in $n$ draws, without replacement, from a finite population of size $N$, where exactly $K$ members are successes. (In contrast, a binomial random variable can be thought of as describing the probability of $k$ successes with replacement.)
		
			\noindent Properties of Hypergeometric Random Variables:
		\begin{multicols}{3}
			\begin{itemize}
				\item $P(X=k) = \dfrac{{K \choose k} {{N-k} \choose {n-k}}}{{N \choose n}}$
				\item $E[X] = \frac{nK}{N}$
				\item $\text{Var}(X)= n \frac{K}{N} \frac{N-K}{N} \frac{N-n}{n-1}$
			\end{itemize}
		\end{multicols}
		\noindent Note: $\text{Var}(X) \approx n(p)(1-p)$ when $N$ is large relative to $n$
		
		\subsubsection*{Expected Value and Sums of Random Variables}
		If $X_1, X_2, \ldots, X_n$ are random variables then
		\begin{align*}
			E \left[ \sum_{i=1}^{n} X_i \right] = \sum_{i=1}^{n} E[X_i]
		\end{align*}
\end{document}